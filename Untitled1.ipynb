{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from graph import *\n",
    "import walker\n",
    "import os, sys, time, pickle\n",
    "from collections import Counter\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/home/jehyuk/Documents/OpenNE/data/cora/cora_edgelist.txt'\n",
    "walker_file = '/home/jehyuk/Documents/OpenNE/tmp/cora_walker.obj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rw(input_file, walker_file):\n",
    "    graph = Graph()\n",
    "    graph.read_adjlist(input_file)\n",
    "    rw_gen = walker.BasicWalker(graph, workers = 4)\n",
    "    walks = rw_gen.simulate_walks(num_walks = 5, walk_length = 5)\n",
    "    with open(walker_file, 'wb') as f:\n",
    "        pickle.dump(walks, f)\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rw(rw_file):\n",
    "    with open(rw_file, 'rb') as f:\n",
    "        rw_data = pickle.load(f)\n",
    "    return rw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(total_data):\n",
    "    word_counts = Counter(total_data)\n",
    "    int_to_vocab = dict()\n",
    "    vocab_to_int = dict()\n",
    "    for key in word_counts.keys():\n",
    "        int_to_vocab[int(key)] = key\n",
    "    for int_key in range(max([int(x) for x in word_counts.keys()])):\n",
    "         if int_key not in int_to_vocab.keys():\n",
    "            int_to_vocab[int_key] = str(int_key)\n",
    "    for key in int_to_vocab.keys():\n",
    "        vocab_to_int[str(key)] = key\n",
    "    return vocab_to_int, int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utils_get_batches(int_text, batch_size, seq_length):\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "    \n",
    "    xdata = np.array(int_text[:n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1:n_batches * batch_size * seq_length + 1])\n",
    "    \n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "    \n",
    "    return list(zip(x_batches, y_batches))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_data = read_rw(walker_file)\n",
    "total_data = list()\n",
    "for i in range(len(rw_data)):\n",
    "    total_data.extend((x) for x in rw_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(total_data)\n",
    "int_words = [vocab_to_int[word] for word in total_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs          = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop        = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1-p_drop[word])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Mikolov et al.:\n",
    "\n",
    "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C = 5$, for each training word we will select randomly a number $R$ in range $&lt; 1; C &gt;$, and then use $R$ words from history and $R$ words from the future of the current word as correct labels.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(words, idx, window_size = 5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx-R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(words, batch_size, window_size=5):\n",
    "    n_batches = len(words) // batch_size\n",
    "    \n",
    "    words = words[:n_batches * batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x = list()\n",
    "        y = list()\n",
    "        batch = words[idx: idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x] * len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(int_to_vocab)\n",
    "embed_dim = 50\n",
    "negative_samples = 100\n",
    "max_epochs = 1000\n",
    "batch_size = 100\n",
    "window_size = 5\n",
    "\n",
    "valid_size=4\n",
    "valid_window = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-3cc82ac4185a>:26: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Epoch 50 / 1000, Iterations: 5195, Avg trn loss: 287.7568, 0.0027 s/batch\n",
      "Nearest to 69:  670,  1820,  59,  2217,  628,  938,  1796,  385, \n",
      "Nearest to 3:  392,  2631,  977,  2556,  2573,  509,  915,  2374, \n",
      "Nearest to 1014:  867,  495,  1715,  2171,  2500,  2705,  2673,  148, \n",
      "Nearest to 1023:  2273,  1671,  1467,  2586,  470,  1618,  1617,  1474, \n",
      "\n",
      "Epoch 100 / 1000, Iterations: 10495, Avg trn loss: 153.1273, 0.0033 s/batch\n",
      "Nearest to 69:  911,  670,  1583,  675,  165,  2600,  938,  2214, \n",
      "Nearest to 3:  2544,  430,  1543,  1028,  855,  1702,  1336,  2116, \n",
      "Nearest to 1014:  2375,  495,  2171,  1258,  867,  486,  869,  90, \n",
      "Nearest to 1023:  1467,  1657,  2586,  1474,  2290,  641,  859,  725, \n",
      "\n",
      "Epoch 150 / 1000, Iterations: 15795, Avg trn loss: 104.8127, 0.0034 s/batch\n",
      "Nearest to 69:  911,  1583,  2600,  1532,  165,  670,  2294,  1594, \n",
      "Nearest to 3:  2544,  430,  1028,  1543,  1688,  855,  774,  1702, \n",
      "Nearest to 1014:  2375,  1351,  2269,  1404,  1330,  495,  2171,  2545, \n",
      "Nearest to 1023:  1801,  1467,  1479,  1657,  490,  787,  360,  641, \n",
      "\n",
      "Epoch 200 / 1000, Iterations: 21095, Avg trn loss: 83.4066, 0.0035 s/batch\n",
      "Nearest to 69:  911,  1583,  1594,  2294,  369,  675,  2012,  981, \n",
      "Nearest to 3:  2544,  430,  1028,  774,  1448,  1688,  1920,  399, \n",
      "Nearest to 1014:  2375,  1351,  744,  1191,  259,  2030,  732,  279, \n",
      "Nearest to 1023:  1479,  1801,  2415,  950,  1467,  725,  490,  694, \n",
      "\n",
      "Epoch 250 / 1000, Iterations: 26395, Avg trn loss: 72.4013, 0.0030 s/batch\n",
      "Nearest to 69:  1583,  2214,  1291,  236,  1594,  2155,  369,  1869, \n",
      "Nearest to 3:  2544,  430,  774,  1028,  1448,  1920,  399,  831, \n",
      "Nearest to 1014:  2375,  744,  1351,  1191,  259,  1747,  1915,  1330, \n",
      "Nearest to 1023:  1479,  950,  2415,  1801,  1467,  54,  725,  1522, \n",
      "\n",
      "Epoch 300 / 1000, Iterations: 31695, Avg trn loss: 65.8379, 0.0028 s/batch\n",
      "Nearest to 69:  39,  443,  1291,  2155,  1583,  369,  1594,  2214, \n",
      "Nearest to 3:  2544,  430,  1028,  774,  1448,  831,  278,  1539, \n",
      "Nearest to 1014:  2375,  744,  1191,  1351,  259,  1915,  2539,  1747, \n",
      "Nearest to 1023:  950,  1479,  2415,  1467,  1522,  1801,  1342,  54, \n",
      "\n",
      "Epoch 350 / 1000, Iterations: 36995, Avg trn loss: 61.5527, 0.0027 s/batch\n",
      "Nearest to 69:  39,  2155,  2214,  1869,  443,  911,  1291,  236, \n",
      "Nearest to 3:  2544,  430,  1028,  278,  774,  831,  1858,  1543, \n",
      "Nearest to 1014:  744,  1351,  1191,  1915,  2375,  259,  109,  1747, \n",
      "Nearest to 1023:  950,  1479,  2415,  1467,  1522,  1342,  725,  1316, \n",
      "\n",
      "Epoch 400 / 1000, Iterations: 42295, Avg trn loss: 59.0301, 0.0030 s/batch\n",
      "Nearest to 69:  2155,  39,  1291,  443,  1869,  2214,  911,  236, \n",
      "Nearest to 3:  2544,  278,  831,  430,  774,  1858,  1539,  2300, \n",
      "Nearest to 1014:  744,  1191,  259,  1351,  1915,  2375,  1747,  2244, \n",
      "Nearest to 1023:  950,  1479,  2415,  1467,  1522,  725,  1316,  54, \n",
      "\n",
      "Epoch 450 / 1000, Iterations: 47595, Avg trn loss: 56.8311, 0.0034 s/batch\n",
      "Nearest to 69:  1869,  2155,  1037,  39,  443,  1291,  1583,  2022, \n",
      "Nearest to 3:  2544,  1858,  831,  278,  430,  774,  2300,  1539, \n",
      "Nearest to 1014:  744,  1351,  259,  1191,  1915,  1747,  1864,  2244, \n",
      "Nearest to 1023:  1479,  950,  2415,  1522,  725,  1467,  54,  325, \n",
      "\n",
      "Epoch 500 / 1000, Iterations: 52895, Avg trn loss: 55.1812, 0.0033 s/batch\n",
      "Nearest to 69:  1869,  911,  2155,  443,  2214,  1291,  1858,  2022, \n",
      "Nearest to 3:  2544,  278,  831,  1858,  1539,  430,  774,  2557, \n",
      "Nearest to 1014:  744,  1191,  259,  1351,  2244,  1747,  1915,  1347, \n",
      "Nearest to 1023:  1479,  950,  2415,  1522,  725,  325,  54,  1467, \n",
      "\n",
      "Epoch 550 / 1000, Iterations: 58195, Avg trn loss: 53.8703, 0.0031 s/batch\n",
      "Nearest to 69:  1869,  2155,  443,  1728,  2149,  2214,  1037,  39, \n",
      "Nearest to 3:  2544,  278,  831,  1539,  1858,  2300,  2557,  430, \n",
      "Nearest to 1014:  744,  1747,  1191,  1351,  259,  2244,  1915,  1347, \n",
      "Nearest to 1023:  950,  1479,  2415,  1522,  725,  325,  1467,  54, \n",
      "\n",
      "Epoch 600 / 1000, Iterations: 63495, Avg trn loss: 52.5039, 0.0040 s/batch\n",
      "Nearest to 69:  1869,  443,  967,  911,  1728,  2149,  2500,  130, \n",
      "Nearest to 3:  2544,  1539,  278,  1858,  2300,  831,  2557,  774, \n",
      "Nearest to 1014:  744,  2244,  1191,  1347,  1747,  259,  1351,  1915, \n",
      "Nearest to 1023:  950,  1479,  2415,  1522,  725,  1467,  54,  1801, \n",
      "\n",
      "Epoch 650 / 1000, Iterations: 68795, Avg trn loss: 51.6532, 0.0028 s/batch\n",
      "Nearest to 69:  1869,  967,  1728,  130,  443,  1972,  2500,  1141, \n",
      "Nearest to 3:  2544,  278,  1539,  831,  1858,  2300,  430,  2557, \n",
      "Nearest to 1014:  2244,  744,  1351,  259,  1191,  1347,  1747,  2342, \n",
      "Nearest to 1023:  950,  2415,  1479,  1522,  725,  1467,  1801,  54, \n",
      "\n",
      "Epoch 700 / 1000, Iterations: 74095, Avg trn loss: 50.9264, 0.0034 s/batch\n",
      "Nearest to 69:  1869,  967,  1728,  1972,  1037,  443,  130,  2214, \n",
      "Nearest to 3:  2544,  1858,  278,  1539,  831,  2557,  2300,  430, \n",
      "Nearest to 1014:  744,  2244,  1351,  259,  1191,  1347,  1747,  2342, \n",
      "Nearest to 1023:  950,  1479,  2415,  1522,  725,  42,  1467,  54, \n",
      "\n",
      "Epoch 750 / 1000, Iterations: 79395, Avg trn loss: 50.3400, 0.0031 s/batch\n",
      "Nearest to 69:  1869,  967,  1972,  1728,  443,  130,  1141,  1037, \n",
      "Nearest to 3:  2544,  1539,  831,  278,  1858,  2300,  717,  1635, \n",
      "Nearest to 1014:  744,  1351,  2244,  1347,  259,  1191,  1747,  127, \n",
      "Nearest to 1023:  950,  1479,  2415,  725,  325,  42,  1467,  54, \n",
      "\n",
      "Epoch 800 / 1000, Iterations: 84695, Avg trn loss: 49.6859, 0.0029 s/batch\n",
      "Nearest to 69:  1869,  967,  130,  1728,  1037,  443,  1972,  1141, \n",
      "Nearest to 3:  2544,  278,  1858,  1539,  831,  2300,  2557,  2089, \n",
      "Nearest to 1014:  2244,  744,  1351,  1347,  259,  1191,  1747,  127, \n",
      "Nearest to 1023:  950,  1479,  2415,  725,  42,  1522,  325,  54, \n",
      "\n",
      "Epoch 850 / 1000, Iterations: 89995, Avg trn loss: 49.5078, 0.0028 s/batch\n",
      "Nearest to 69:  1869,  1728,  967,  443,  1141,  1972,  130,  2149, \n",
      "Nearest to 3:  2544,  1539,  278,  1858,  831,  2557,  418,  717, \n",
      "Nearest to 1014:  2244,  744,  259,  1351,  1191,  1347,  1414,  1747, \n",
      "Nearest to 1023:  950,  1479,  2415,  42,  1522,  54,  325,  725, \n",
      "\n",
      "Epoch 900 / 1000, Iterations: 95295, Avg trn loss: 48.6198, 0.0029 s/batch\n",
      "Nearest to 69:  1869,  1728,  967,  443,  1972,  2058,  130,  1141, \n",
      "Nearest to 3:  2544,  278,  1539,  1858,  2557,  418,  717,  831, \n",
      "Nearest to 1014:  2244,  744,  1191,  259,  1347,  1414,  1351,  1747, \n",
      "Nearest to 1023:  2415,  950,  1479,  42,  54,  725,  325,  1522, \n",
      "\n",
      "Epoch 950 / 1000, Iterations: 100595, Avg trn loss: 48.1885, 0.0030 s/batch\n",
      "Nearest to 69:  1869,  1728,  967,  443,  1972,  130,  395,  2149, \n",
      "Nearest to 3:  2544,  1539,  278,  418,  1858,  717,  2557,  2300, \n",
      "Nearest to 1014:  744,  2244,  1347,  1351,  259,  1191,  1414,  1747, \n",
      "Nearest to 1023:  950,  2415,  1479,  42,  54,  1467,  160,  1801, \n",
      "\n",
      "Epoch 1000 / 1000, Iterations: 105895, Avg trn loss: 47.4265, 0.0027 s/batch\n",
      "Nearest to 69:  1869,  1728,  1972,  130,  443,  967,  395,  1141, \n",
      "Nearest to 3:  2544,  1539,  278,  418,  1858,  2557,  673,  717, \n",
      "Nearest to 1014:  2244,  744,  1347,  1191,  259,  1414,  1351,  1747, \n",
      "Nearest to 1023:  950,  1479,  2415,  42,  1467,  725,  54,  1522, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_graph = tf.Graph()\n",
    "\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "    embedding_matrix = tf.Variable(tf.random_uniform((n_vocab, embed_dim), -1, 1))\n",
    "    embedded = tf.nn.embedding_lookup(embedding_matrix, inputs)\n",
    "    # For negative sampling\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, embed_dim)))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab), name = 'softmax_bias')\n",
    "    loss = tf.nn.sampled_softmax_loss(weights = softmax_w, \n",
    "                                      biases = softmax_b, \n",
    "                                      labels = labels, \n",
    "                                      inputs=embedded, \n",
    "                                      num_sampled=negative_samples, \n",
    "                                      num_classes=n_vocab)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    opt = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, random.sample(range(1000,1000+valid_window), valid_size //2))\n",
    "    valid_dataset    = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding_matrix), 1, keep_dims=True))\n",
    "    normalized_embedding_matrix = embedding_matrix / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding_matrix, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding_matrix))\n",
    "      \n",
    "    \n",
    "    \n",
    "with tf.Session(graph = train_graph, config=sess_config) as sess:\n",
    "    _iter = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(1,max_epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            feed_dict = {inputs: x, labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, opt], feed_dict = feed_dict)\n",
    "            loss += train_loss\n",
    "            _iter+=1\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(\"Epoch {} / {}, Iterations: {}, Avg trn loss: {:.4f}, {:.4f} s/batch\".format(epoch+1, max_epochs, _iter, loss/100, (time.time()-start)/100))\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_node = int_to_vocab[valid_examples[i]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[i,:]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s: ' % valid_node\n",
    "                for k in range(top_k):\n",
    "                    close_word = int_to_vocab[nearest[k]]\n",
    "                    log = \"%s %s, \" %(log, close_word)\n",
    "                print(log)\n",
    "\n",
    "            loss = 0\n",
    "            start = time.time()\n",
    "\n",
    "            save_path = saver.save(sess, 'checkpoints/embedded.ckpt')\n",
    "            embed_mat = sess.run(normalized_embedding_matrix)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "graph.read_adjlist(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_26 = [x for x in graph.G.neighbors('26')]\n",
    "x_171 = [x for x in graph.G.neighbors('171')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2454', '2455', '123', '99', '122']\n",
      "['790', '1548', '775']\n"
     ]
    }
   ],
   "source": [
    "print(x_26)\n",
    "print(x_171)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
